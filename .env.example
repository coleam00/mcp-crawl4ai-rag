# The transport for the MCP server - either 'sse' or 'stdio' (defaults to sse if left empty)
TRANSPORT=

# Host to bind to if using sse as the transport (leave empty if using stdio)
HOST=

# Port to listen on if using sse as the transport (leave empty if using stdio)
PORT=

EMBEDDINGS_PROVIDER=bedrock

# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# This is for the embedding model - text-embed-small-3 will be used
OPENAI_API_KEY=

# The LLM you want to use for contextual embeddings (contextual retrieval)
# Leave this blank if you do not want to use contextual embeddings
# Generally this is a very cheap and fast LLM like gpt-4.1-nano
MODEL_CHOICE=

# Provider for contextual information generation ("openai" or "bedrock")
# If "bedrock", ensure BEDROCK_CONTEXT_MODEL_ID is also set.
# Defaults to "openai" if MODEL_CHOICE is set and this is empty.
CONTEXT_PROVIDER=

# Bedrock Model ID for contextual information generation (e.g., "anthropic.claude-v2")
# Required if CONTEXT_PROVIDER is "bedrock".
BEDROCK_CONTEXT_MODEL_ID=

# For the Supabase version (sample_supabase_agent.py), set your Supabase URL and Service Key.
# Get your SUPABASE_URL from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY=

# AÃ±ade variables AWS para Bedrock Titan embeddings
AWS_ACCESS_KEY=
AWS_ACCESS_SECRET=
AWS_REGION=