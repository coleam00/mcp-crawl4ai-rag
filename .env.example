# =================================================================
#  SERVER CONFIGURATION
# =================================================================

# --- Basic Server Settings ---
HOST=0.0.0.0
PORT=8051
TRANSPORT=sse # Options: 'sse' or 'stdio'

# =================================================================
#  DATABASE CONNECTIONS
# =================================================================

# --- Vector Database (Supabase) - REQUIRED ---
SUPABASE_URL=your_supabase_project_url
SUPABASE_SERVICE_KEY=your_supabase_service_key

# --- Knowledge Graph Database (Neo4j) - Optional ---
# Required only if USE_KNOWLEDGE_GRAPH=true
# IMPORTANT: If running via Docker, change localhost to host.docker.internal
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# =================================================================
#  AI MODEL CONFIGURATION
# =================================================================

# --- Primary Models ---
CHAT_MODEL=gpt-4o-mini
CHAT_MODEL_API_KEY=your_chat_model_api_key
CHAT_MODEL_API_BASE= # Optional: e.g., http://localhost:11434/v1 for Ollama

EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_MODEL_API_KEY=your_embedding_model_api_key
EMBEDDING_MODEL_API_BASE= # Optional: e.g., http://localhost:11434/v1 for Ollama
EMBEDDING_DIMENSIONS=1536 # Model native dimensions

# --- Fallback Models (for reliability) ---
CHAT_MODEL_FALLBACK=gpt-3.5-turbo
CHAT_MODEL_FALLBACK_API_KEY=your_fallback_api_key
CHAT_MODEL_FALLBACK_API_BASE= # Optional fallback base URL

EMBEDDING_MODEL_FALLBACK=text-embedding-3-small
EMBEDDING_MODEL_FALLBACK_API_KEY=your_fallback_embedding_api_key
EMBEDDING_MODEL_FALLBACK_API_BASE= # Optional fallback base URL
EMBEDDING_DIMENSIONS_FALLBACK=1536 # Fallback model dimensions

# --- Local Ollama Configuration (Alternative Setup) ---
# Uncomment these settings when using Ollama for local inference
# CHAT_MODEL=qwen3:latest
# CHAT_MODEL_API_BASE=http://localhost:11434/v1  # Use http://ollama:11434/v1 in Docker
# CHAT_MODEL_API_KEY=ollama  # Required but ignored by Ollama

# EMBEDDING_MODEL=dengcao/Qwen3-Embedding-0.6B:Q8_0  # Recommended: compact & efficient (639MB)
# EMBEDDING_MODEL_API_BASE=http://localhost:11434/v1  # Use http://ollama:11434/v1 in Docker
# EMBEDDING_MODEL_API_KEY=ollama  # Required but ignored by Ollama
# EMBEDDING_DIMENSIONS=1024  # Native for 0.6B model

# --- Optional Models ---
RERANKING_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# =================================================================
#  FEATURE FLAGS & RAG STRATEGIES
# =================================================================

# --- RAG Enhancement Features (Default: false) ---
USE_CONTEXTUAL_EMBEDDINGS=false  # LLM-generated context for better embeddings
USE_HYBRID_SEARCH=false          # Combine vector + keyword search
USE_AGENTIC_RAG=false            # Extract and index code examples separately
USE_RERANKING=false              # Cross-encoder reranking for relevance
USE_KNOWLEDGE_GRAPH=false        # Neo4j-based code analysis and validation

# --- Reliability Features (Default: false) ---
USE_CHAT_MODEL_FALLBACK=false      # Auto-fallback for chat model failures
USE_EMBEDDING_MODEL_FALLBACK=false # Auto-fallback for embedding model failures

# =================================================================
#  PERFORMANCE & OPTIMIZATION
# =================================================================

# --- Content Processing ---
MAX_CRAWL_DEPTH=3
MAX_CONCURRENT_CRAWLS=10
CHUNK_SIZE=5000
MIN_CODE_BLOCK_LENGTH=1000 # Minimum characters for code extraction
DEFAULT_MATCH_COUNT=5

# --- Concurrency Control (Legacy) ---
MAX_WORKERS_SUMMARY=1        # Code example summarization workers
MAX_WORKERS_CONTEXT=1        # Contextual embedding workers  
MAX_WORKERS_SOURCE_SUMMARY=1 # Source summarization workers
SUPABASE_BATCH_SIZE=2        # Database batch insert size

# --- Rate Limiting & Circuit Breaker (New: Anti-503 System) ---
MAX_CONCURRENT_REQUESTS=5     # Global simultaneous request limit
REQUEST_TIMEOUT=30            # Per-request timeout (seconds)
RATE_LIMIT_DELAY=0.5         # Minimum delay between requests (seconds)
CIRCUIT_BREAKER_THRESHOLD=3   # Failures before triggering fallback
CLIENT_CACHE_TTL=3600        # OpenAI client cache duration (seconds)

# =================================================================
#  DEPRECATED
# =================================================================
# This key is kept for backward compatibility but will be removed in the future.
# Use model-specific keys (CHAT_MODEL_API_KEY, EMBEDDING_MODEL_API_KEY) instead.
# OPENAI_API_KEY=your_openai_api_key