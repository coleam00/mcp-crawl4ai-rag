# =================================================================
#  CORE CONFIGURATION
# =================================================================

# --- Server Settings ---
# Configure the basic server connection
HOST=0.0.0.0
PORT=8051
TRANSPORT=sse # 'sse' or 'stdio'

# --- Service Credentials ---
# API keys and connection strings for external services

# Chat Model (for summaries, contextual embeddings, etc.)
CHAT_MODEL_API_KEY=your_chat_model_api_key
CHAT_MODEL_API_BASE= # Optional: e.g., http://localhost:11434/v1 for Ollama

# Fallback Chat Model (automatically used if primary model fails)
CHAT_MODEL_FALLBACK=gpt-3.5-turbo
CHAT_MODEL_FALLBACK_API_KEY=your_fallback_api_key
CHAT_MODEL_FALLBACK_API_BASE= # Optional fallback base URL

# Embedding Model
EMBEDDING_MODEL_API_KEY=your_embedding_model_api_key
EMBEDDING_MODEL_API_BASE= # Optional: e.g., http://localhost:11434/v1 for Ollama

# Fallback Embedding Model (automatically used if primary embedding model fails)
EMBEDDING_MODEL_FALLBACK=text-embedding-3-small
EMBEDDING_MODEL_FALLBACK_API_KEY=your_fallback_embedding_api_key
EMBEDDING_MODEL_FALLBACK_API_BASE= # Optional fallback base URL
EMBEDDING_DIMENSIONS_FALLBACK=1536 # Dimensions for fallback embedding model

# --- Local Ollama Configuration (Alternative to API-based models) ---
# Uncomment these settings when using Ollama for local inference
# CHAT_MODEL_API_BASE=http://localhost:11434/v1  # Use http://host.docker.internal:11434/v1 if running in Docker
# CHAT_MODEL_API_KEY=ollama  # Required but ignored by Ollama
# EMBEDDING_MODEL_API_BASE=http://localhost:11434/v1  # Use http://host.docker.internal:11434/v1 if running in Docker  
# EMBEDDING_MODEL_API_KEY=ollama  # Required but ignored by Ollama

# Vector Database (Supabase)
SUPABASE_URL=your_supabase_project_url
SUPABASE_SERVICE_KEY=your_supabase_service_key

# Knowledge Graph Database (Neo4j)
# Required if USE_KNOWLEDGE_GRAPH=true
# IMPORTANT: If running via Docker, change localhost to host.docker.internal
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# =================================================================
#  RAG STRATEGY & FEATURE FLAGS
# =================================================================
# Enable or disable specific RAG features. All default to "false".

# Use an LLM to generate richer, more contextual embeddings for document chunks.
# Improves retrieval accuracy at the cost of slower indexing.
USE_CONTEXTUAL_EMBEDDINGS=false

# Combine vector similarity search with traditional keyword search.
USE_HYBRID_SEARCH=false

# Extract code examples from crawled content and enable specialized code search.
USE_AGENTIC_RAG=false

# Apply a cross-encoder model to rerank search results for better relevance.
USE_RERANKING=false

# Enable knowledge graph tools for repository analysis and hallucination detection.
USE_KNOWLEDGE_GRAPH=false

# Enable automatic fallback to backup chat model on API failures (503, 429, timeout, etc.).
USE_CHAT_MODEL_FALLBACK=false

# Enable automatic fallback to backup embedding model on API failures.
USE_EMBEDDING_MODEL_FALLBACK=false


# =================================================================
#  ADVANCED CONFIGURATION & FINE-TUNING
# =================================================================
# Tweak these parameters to optimize performance and behavior.

# --- Model Selection ---
CHAT_MODEL=gpt-4.1-mini
EMBEDDING_MODEL="text-embedding-3-small"
RERANKING_MODEL="cross-encoder/ms-marco-MiniLM-L-6-v2"

# --- Local Ollama Model Selection (Recommended) ---
# Use these models when running with Ollama for privacy and cost efficiency
# CHAT_MODEL=qwen3:latest  # or qwen3:7b, qwen3:14b, etc.
# EMBEDDING_MODEL=dengcao/Qwen3-Embedding-0.6B:Q8_0  # Recommended: compact & efficient (639MB)
# EMBEDDING_MODEL=dengcao/Qwen3-Embedding-8B:Q4_K_M  # Alternative: higher capacity (4.7GB)

# --- Crawling & Content Processing ---
MAX_CRAWL_DEPTH=3
MAX_CONCURRENT_CRAWLS=10
CHUNK_SIZE=5000
MIN_CODE_BLOCK_LENGTH=1000 # Minimum characters for a code block to be extracted

# --- Search & Embedding ---
DEFAULT_MATCH_COUNT=5
EMBEDDING_DIMENSIONS=1024  # Native for Qwen3-Embedding-0.6B, up to 4096 for Qwen3-Embedding-8B

# --- Concurrency & Batching (Optimized to prevent 503 errors) ---
MAX_WORKERS_SUMMARY=1        # For summarizing code examples (reduced for stability)
MAX_WORKERS_CONTEXT=1        # For generating contextual embeddings (prevents API overload)
MAX_WORKERS_SOURCE_SUMMARY=1 # For summarizing entire sources (conservative setting)
SUPABASE_BATCH_SIZE=2        # Smaller batches for better stability

# --- Rate Limiting & Circuit Breaker (New: Prevents 503 Service Unavailable) ---
MAX_CONCURRENT_REQUESTS=5     # Maximum simultaneous API requests
REQUEST_TIMEOUT=30            # Request timeout in seconds
RATE_LIMIT_DELAY=0.5         # Minimum delay between requests (seconds)
CIRCUIT_BREAKER_THRESHOLD=3   # Number of consecutive failures to trigger circuit breaker
CLIENT_CACHE_TTL=3600        # Client cache time-to-live in seconds (1 hour)

# =================================================================
#  DEPRECATED
# =================================================================
# This key is kept for backward compatibility but will be removed in the future.
# Use model-specific keys (CHAT_MODEL_API_KEY, EMBEDDING_MODEL_API_KEY) instead.
# OPENAI_API_KEY=your_openai_api_key