# =================================================================
#  DEFAULT VALUES
# =================================================================

# --- System Defaults ---
# These values are used as fallbacks when environment variables are not set
DEFAULT_HOST=0.0.0.0
DEFAULT_PORT=8051
OLLAMA_CHECK_TIMEOUT=5
REPO_ANALYSIS_TIMEOUT=1800
DEFAULT_MAX_RETRIES=3

# =================================================================
#  SERVER CONFIGURATION
# =================================================================

# --- Basic Server Settings ---
HOST=0.0.0.0
PORT=8051
TRANSPORT=sse # Options: 'sse' or 'stdio'

# =================================================================
#  DATABASE CONNECTIONS
# =================================================================

# --- Vector Database (Supabase) - REQUIRED ---
SUPABASE_URL=your_supabase_project_url
SUPABASE_SERVICE_KEY=your_supabase_service_key

# --- Knowledge Graph Database (Neo4j) - Optional ---
# Required only if USE_KNOWLEDGE_GRAPH=true
# IMPORTANT: If running via Docker, change localhost to host.docker.internal
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# =================================================================
#  AI MODEL CONFIGURATION
# =================================================================

# --- Primary Models ---
# NOTE: These models are now configurable (previously hardcoded)
CHAT_MODEL=gpt-4o-mini
CHAT_MODEL_API_KEY=your_chat_model_api_key
CHAT_MODEL_API_BASE= # Optional: e.g., http://localhost:11434/v1 for Ollama

EMBEDDING_MODEL=text-embedding-3-small  # Now configurable (was hardcoded)
EMBEDDING_MODEL_API_KEY=your_embedding_model_api_key
EMBEDDING_MODEL_API_BASE= # Optional: e.g., http://localhost:11434/v1 for Ollama
EMBEDDING_DIMENSIONS=1536 # Now configurable (was hardcoded)

# --- Fallback Models (for reliability) ---
CHAT_MODEL_FALLBACK=gpt-3.5-turbo
CHAT_MODEL_FALLBACK_API_KEY=your_fallback_api_key
CHAT_MODEL_FALLBACK_API_BASE= # Optional fallback base URL

EMBEDDING_MODEL_FALLBACK=text-embedding-3-small
EMBEDDING_MODEL_FALLBACK_API_KEY=your_fallback_embedding_api_key
EMBEDDING_MODEL_FALLBACK_API_BASE= # Optional fallback base URL
EMBEDDING_DIMENSIONS_FALLBACK=1536 # Fallback model dimensions

# --- Local Ollama Configuration (Alternative Setup) ---
# Uncomment these settings when using Ollama for local inference
# CHAT_MODEL=qwen3:latest
# CHAT_MODEL_API_BASE=http://localhost:11434/v1  # Use http://ollama:11434/v1 in Docker
# CHAT_MODEL_API_KEY=ollama  # Required but ignored by Ollama

# EMBEDDING_MODEL=dengcao/Qwen3-Embedding-0.6B:Q8_0  # Recommended: compact & efficient (639MB)
# EMBEDDING_MODEL_API_BASE=http://localhost:11434/v1  # Use http://ollama:11434/v1 in Docker
# EMBEDDING_MODEL_API_KEY=ollama  # Required but ignored by Ollama
# EMBEDDING_DIMENSIONS=1024  # Native for 0.6B model

# --- Optional Models ---
RERANKING_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2  # Now configurable (was hardcoded)

# =================================================================
#  FEATURE FLAGS & RAG STRATEGIES
# =================================================================

# --- RAG Enhancement Features (Default: false) ---
USE_CONTEXTUAL_EMBEDDINGS=false  # LLM-generated context for better embeddings
USE_HYBRID_SEARCH=false          # Combine vector + keyword search
USE_AGENTIC_RAG=false            # Extract and index code examples separately
USE_RERANKING=false              # Cross-encoder reranking for relevance
USE_KNOWLEDGE_GRAPH=false        # Neo4j-based code analysis and validation

# --- Reliability Features (Default: false) ---
USE_CHAT_MODEL_FALLBACK=false      # Auto-fallback for chat model failures
USE_EMBEDDING_MODEL_FALLBACK=false # Auto-fallback for embedding model failures

# =================================================================
#  PERFORMANCE & OPTIMIZATION
# =================================================================

# --- Content Processing ---
MAX_CRAWL_DEPTH=3
MAX_CONCURRENT_CRAWLS=10
CHUNK_SIZE=5000
MIN_CODE_BLOCK_LENGTH=1000 # Minimum characters for code extraction
DEFAULT_MATCH_COUNT=5

# --- Concurrency Control (Legacy) ---
MAX_WORKERS_SUMMARY=1        # Code example summarization workers
MAX_WORKERS_CONTEXT=1        # Contextual embedding workers  
MAX_WORKERS_SOURCE_SUMMARY=1 # Source summarization workers
SUPABASE_BATCH_SIZE=2        # Database batch insert size
SUPABASE_MAX_RETRIES=3       # Database operation retry attempts

# --- Rate Limiting & Circuit Breaker (New: Anti-503 System) ---
MAX_CONCURRENT_REQUESTS=5     # Global simultaneous request limit
REQUEST_TIMEOUT=30            # Per-request timeout (seconds)
RATE_LIMIT_DELAY=0.5         # Minimum delay between requests (seconds)
CIRCUIT_BREAKER_THRESHOLD=3   # Failures before triggering fallback
CLIENT_CACHE_TTL=3600        # OpenAI client cache duration (seconds)

# =================================================================
#  CONFIGURATION DOCUMENTATION
# =================================================================

# --- Container vs Development Behavior ---
# When running in Docker containers:
# - Use service names (e.g., http://ollama:11434/v1) instead of localhost
# - Environment variables override defaults automatically
# - Health checks and service discovery are handled automatically

# When running in development (uv/pip):
# - Use localhost URLs (e.g., http://localhost:11434/v1)
# - Ensure services are running locally before starting the MCP server
# - Use .env file for configuration (copy from .env.example)

# --- Migration Guide from Previous Version ---
# BREAKING CHANGES:
# 1. EMBEDDING_MODEL, EMBEDDING_DIMENSIONS, RERANKING_MODEL are now configurable
#    - Previously these were hardcoded in the source code
#    - Update your .env file to specify these values explicitly
# 2. New rate limiting system replaces simple concurrency controls
#    - MAX_CONCURRENT_REQUESTS replaces some worker pool limits
#    - RATE_LIMIT_DELAY adds minimum delays between API calls
# 3. Fallback system now has separate enable flags
#    - Set USE_CHAT_MODEL_FALLBACK=true to enable chat fallback
#    - Set USE_EMBEDDING_MODEL_FALLBACK=true to enable embedding fallback

# --- Configuration Notes by Section ---
# DEFAULT VALUES: System-wide fallbacks, usually don't need changing
# SERVER: Basic connectivity settings
# DATABASE: Required for vector storage (Supabase) and optional knowledge graphs (Neo4j)
# AI MODELS: Configure your primary and fallback models here
# FEATURE FLAGS: Enable/disable RAG enhancement strategies
# PERFORMANCE: Tune for your hardware and API rate limits

# =================================================================
#  CHANGELOG
# =================================================================

# Version 2.0 (Rate Limiting & Circuit Breaker Update):
# ADDED:
# - DEFAULT_HOST, DEFAULT_PORT, OLLAMA_CHECK_TIMEOUT, REPO_ANALYSIS_TIMEOUT, DEFAULT_MAX_RETRIES
# - MAX_CONCURRENT_REQUESTS, RATE_LIMIT_DELAY, CIRCUIT_BREAKER_THRESHOLD, REQUEST_TIMEOUT, CLIENT_CACHE_TTL
# - Configurable EMBEDDING_MODEL, EMBEDDING_DIMENSIONS, RERANKING_MODEL (were hardcoded)
# - USE_CHAT_MODEL_FALLBACK, USE_EMBEDDING_MODEL_FALLBACK flags
# CHANGED:
# - Improved fallback system with individual model control
# - Enhanced rate limiting with circuit breaker protection
# DEPRECATED:
# - OPENAI_API_KEY (use model-specific keys instead)

# =================================================================
#  DEPRECATED
# =================================================================
# OPENAI_API_KEY=your_openai_api_key
# This key is deprecated. Use model-specific keys (CHAT_MODEL_API_KEY, EMBEDDING_MODEL_API_KEY) instead.
# Will be removed in a future version.