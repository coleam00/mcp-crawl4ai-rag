# =================================================================
#  CORE CONFIGURATION
# =================================================================

# --- Server Settings ---
# Configure the basic server connection
HOST=0.0.0.0
PORT=8051
TRANSPORT=sse # 'sse' or 'stdio'

# --- Service Credentials ---
# API keys and connection strings for external services

# Chat Model (for summaries, contextual embeddings, etc.)
CHAT_MODEL_API_KEY=your_chat_model_api_key
CHAT_MODEL_API_BASE= # Optional: e.g., http://localhost:11434/v1 for Ollama

# Embedding Model
EMBEDDING_MODEL_API_KEY=your_embedding_model_api_key
EMBEDDING_MODEL_API_BASE= # Optional: e.g., http://localhost:11434/v1 for Ollama

# --- Local Ollama Configuration (Alternative to API-based models) ---
# Uncomment these settings when using Ollama for local inference
# CHAT_MODEL_API_BASE=http://localhost:11434/v1  # Use http://host.docker.internal:11434/v1 if running in Docker
# CHAT_MODEL_API_KEY=ollama  # Required but ignored by Ollama
# EMBEDDING_MODEL_API_BASE=http://localhost:11434/v1  # Use http://host.docker.internal:11434/v1 if running in Docker  
# EMBEDDING_MODEL_API_KEY=ollama  # Required but ignored by Ollama

# Vector Database (Supabase)
SUPABASE_URL=your_supabase_project_url
SUPABASE_SERVICE_KEY=your_supabase_service_key

# Knowledge Graph Database (Neo4j)
# Required if USE_KNOWLEDGE_GRAPH=true
# IMPORTANT: If running via Docker, change localhost to host.docker.internal
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# =================================================================
#  RAG STRATEGY & FEATURE FLAGS
# =================================================================
# Enable or disable specific RAG features. All default to "false".

# Use an LLM to generate richer, more contextual embeddings for document chunks.
# Improves retrieval accuracy at the cost of slower indexing.
USE_CONTEXTUAL_EMBEDDINGS=false

# Combine vector similarity search with traditional keyword search.
USE_HYBRID_SEARCH=false

# Extract code examples from crawled content and enable specialized code search.
USE_AGENTIC_RAG=false

# Apply a cross-encoder model to rerank search results for better relevance.
USE_RERANKING=false

# Enable knowledge graph tools for repository analysis and hallucination detection.
USE_KNOWLEDGE_GRAPH=false


# =================================================================
#  ADVANCED CONFIGURATION & FINE-TUNING
# =================================================================
# Tweak these parameters to optimize performance and behavior.

# --- Model Selection ---
CHAT_MODEL=gpt-4.1-mini
EMBEDDING_MODEL="text-embedding-3-small"
RERANKING_MODEL="cross-encoder/ms-marco-MiniLM-L-6-v2"

# --- Local Ollama Model Selection ---
# Use these models when running with Ollama (uncomment and adjust as needed)
# CHAT_MODEL=qwen3:latest  # or qwen3:7b, qwen3:14b, etc.
# EMBEDDING_MODEL=dengcao/Qwen3-Embedding-8B  # or dengcao/Qwen3-Embedding-0.6B for efficiency

# --- Crawling & Content Processing ---
MAX_CRAWL_DEPTH=3
MAX_CONCURRENT_CRAWLS=10
CHUNK_SIZE=5000
MIN_CODE_BLOCK_LENGTH=1000 # Minimum characters for a code block to be extracted

# --- Search & Embedding ---
DEFAULT_MATCH_COUNT=5
EMBEDDING_DIMENSIONS=1536  # Use 1024 for Qwen3-Embedding-0.6B, up to 4096 for Qwen3-Embedding-8B

# --- Concurrency & Batching ---
MAX_WORKERS_SUMMARY=10       # For summarizing code examples
MAX_WORKERS_CONTEXT=10       # For generating contextual embeddings
MAX_WORKERS_SOURCE_SUMMARY=5 # For summarizing entire sources
SUPABASE_BATCH_SIZE=20

# =================================================================
#  DEPRECATED
# =================================================================
# This key is kept for backward compatibility but will be removed in the future.
# Use model-specific keys (CHAT_MODEL_API_KEY, EMBEDDING_MODEL_API_KEY) instead.
# OPENAI_API_KEY=your_openai_api_key