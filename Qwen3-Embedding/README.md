# Qwen3-Embedding Docker Deploy

Deploy completo do modelo Qwen3-Embedding com API compat√≠vel OpenAI usando vLLM e Docker.

## üöÄ Caracter√≠sticas

- ‚úÖ **API compat√≠vel com OpenAI** - Use com qualquer cliente OpenAI
- ‚úÖ **Suporte a dimens√µes customizadas** - MRL (Matryoshka Representation Learning)
- ‚úÖ **M√∫ltiplos modelos** - 0.6B, 4B e 8B par√¢metros
- ‚úÖ **Deploy assistido** - Scripts automatizados para facilitar o uso
- ‚úÖ **Monitoramento** - Health checks e m√©tricas
- ‚úÖ **Testes autom√°ticos** - Valida√ß√£o completa da API
- ‚úÖ **Multi-GPU** - Suporte a paraleliza√ß√£o tensor

## üìã Pr√©-requisitos

- Docker >= 20.10
- Docker Compose >= 1.28
- NVIDIA GPU (recomendado)
- NVIDIA Container Toolkit (para GPU)
- 8GB+ RAM dispon√≠vel para Docker
- 10GB+ espa√ßo em disco

## ‚ö° In√≠cio R√°pido

### 1. Configura√ß√£o Inicial

```bash
# Clone ou baixe este projeto
cd Qwen3-Embedding

# Copie o arquivo de configura√ß√£o
cp .env.example .env

# Edite as configura√ß√µes se necess√°rio
nano .env
```

### 2. Deploy Autom√°tico

```bash
# Deploy completo com um comando
./scripts/deploy.sh --build --start --test

# Ou use o menu interativo
./scripts/deploy.sh
```

### 3. Teste R√°pido

```bash
# Testar a API
curl -X POST http://localhost:8000/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-Embedding-0.6B",
    "input": ["Ol√° mundo!"],
    "dimensions": 512
  }'
```

## üîß Configura√ß√£o

### Vari√°veis de Ambiente (.env)

```bash
# Modelo principal (op√ß√µes: 0.6B, 4B, 8B)
MODEL_NAME=Qwen/Qwen3-Embedding-0.6B

# Configura√ß√µes de rede
HOST_PORT=8000

# Configura√ß√µes de GPU
GPU_MEMORY_UTIL=0.9
TENSOR_PARALLEL_SIZE=1

# Token do Hugging Face (opcional)
HUGGING_FACE_HUB_TOKEN=your_token_here
```

### Modelos Dispon√≠veis

| Modelo | Tamanho | Dimens√µes Max | Mem√≥ria GPU | Uso Recomendado |
|--------|---------|---------------|-------------|-----------------|
| Qwen3-Embedding-0.6B | 0.6B | 1024 | ~2GB | Desenvolvimento, testes |
| Qwen3-Embedding-4B | 4B | 2560 | ~8GB | Produ√ß√£o balanceada |
| Qwen3-Embedding-8B | 8B | 4096 | ~16GB | M√°xima qualidade |

## üö¶ Comandos Principais

### Scripts de Deploy

```bash
# Deploy completo
./scripts/deploy.sh --build --start

# Construir imagem
./scripts/build.sh

# Iniciar servi√ßos
./scripts/deploy.sh --start

# Parar servi√ßos
./scripts/deploy.sh --stop

# Ver logs
./scripts/deploy.sh --logs

# Executar testes
./scripts/test.sh

# Status dos servi√ßos
./scripts/deploy.sh --status
```

### Docker Compose

```bash
# Iniciar em background
docker-compose up -d

# Ver logs
docker-compose logs -f

# Parar servi√ßos
docker-compose down

# Reconstruir e iniciar
docker-compose up -d --build
```

## üìä Uso da API

### Cliente Python

```python
import openai

# Configurar cliente
client = openai.OpenAI(
    api_key="EMPTY",
    base_url="http://localhost:8000/v1"
)

# Gerar embedding
response = client.embeddings.create(
    model="Qwen/Qwen3-Embedding-0.6B",
    input=["Seu texto aqui"],
    dimensions=512  # Opcional: customizar dimens√µes
)

print(response.data[0].embedding)
```

### Cliente Curl

```bash
# Embedding b√°sico
curl -X POST http://localhost:8000/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-Embedding-0.6B",
    "input": ["Texto para embedding"]
  }'

# M√∫ltiplos textos com dimens√µes customizadas
curl -X POST http://localhost:8000/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-Embedding-0.6B",
    "input": ["Texto 1", "Texto 2", "Texto 3"],
    "dimensions": 768
  }'
```

### Cliente de Exemplo

```bash
# Usar o cliente Python inclu√≠do
python examples/client.py --help

# Gerar embedding
python examples/client.py --text "Seu texto aqui"

# Demo de busca sem√¢ntica
python examples/client.py --demo

# Benchmark de dimens√µes
python examples/client.py --benchmark
```

## üîç Testes e Valida√ß√£o

### Testes Autom√°ticos

```bash
# Executar todos os testes
./scripts/test.sh

# Aguardar servi√ßo e testar
./scripts/test.sh --wait

# Testes incluem:
# - Health check
# - Listagem de modelos
# - Embedding b√°sico
# - Embedding em lote
# - Dimens√µes customizadas
# - Performance
```

### Monitoramento

```bash
# Status dos servi√ßos
./scripts/deploy.sh --status

# Logs em tempo real
./scripts/deploy.sh --logs --follow

# M√©tricas do container
docker stats qwen3-embedding-server
```

## üìÅ Estrutura do Projeto

```
Qwen3-Embedding/
‚îú‚îÄ‚îÄ Dockerfile                 # Imagem Docker
‚îú‚îÄ‚îÄ docker-compose.yml         # Orquestra√ß√£o
‚îú‚îÄ‚îÄ start.sh                   # Script de inicializa√ß√£o
‚îú‚îÄ‚îÄ .env.example              # Configura√ß√µes de exemplo
‚îú‚îÄ‚îÄ README.md                 # Este arquivo
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ build.sh              # Build da imagem
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh             # Deploy completo
‚îÇ   ‚îî‚îÄ‚îÄ test.sh               # Testes autom√°ticos
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îî‚îÄ‚îÄ client.py             # Cliente Python de exemplo
‚îú‚îÄ‚îÄ config/                   # Configura√ß√µes opcionais
‚îú‚îÄ‚îÄ logs/                     # Logs do servi√ßo
‚îî‚îÄ‚îÄ models/                   # Cache de modelos
```

## üîß Configura√ß√µes Avan√ßadas

### Multi-GPU

```bash
# No .env, configure:
TENSOR_PARALLEL_SIZE=2  # Para 2 GPUs
GPU_COUNT=2

# Ou diretamente no docker-compose:
docker-compose up -d --scale qwen3-embedding=1
```

### Modelos Quantizados

```bash
# Usar modelo quantizado AWQ
MODEL_NAME=Qwen/Qwen3-Embedding-0.6B-AWQ
QUANTIZATION=awq
```

### Configura√ß√µes de Performance

```bash
# Otimizar para throughput
ENABLE_PREFIX_CACHING=true
ENABLE_CHUNKED_PREFILL=true
GPU_MEMORY_UTIL=0.95

# Otimizar para lat√™ncia
ENABLE_PREFIX_CACHING=false
DISABLE_LOG_REQUESTS=true
```

## üêõ Solu√ß√£o de Problemas

### Problemas Comuns

**1. Erro: "CUDA out of memory"**
```bash
# Reduza a utiliza√ß√£o de GPU
GPU_MEMORY_UTIL=0.7

# Ou use modelo menor
MODEL_NAME=Qwen/Qwen3-Embedding-0.6B
```

**2. API n√£o responde**
```bash
# Verifique os logs
./scripts/deploy.sh --logs

# Reinicie os servi√ßos
./scripts/deploy.sh --restart
```

**3. Modelo n√£o baixa**
```bash
# Configure token do Hugging Face
HUGGING_FACE_HUB_TOKEN=your_token

# Ou use ModelScope
VLLM_USE_MODELSCOPE=true
```

### Debug Avan√ßado

```bash
# Executar container em modo debug
docker run -it --rm --gpus all \
  -p 8000:8000 \
  qwen3-embedding:latest \
  bash

# Verificar GPU no container
nvidia-smi

# Testar modelo manualmente
python -c "from vllm import LLM; print('OK')"
```

## üìö Documenta√ß√£o Adicional

- [Documenta√ß√£o vLLM](https://docs.vllm.ai/)
- [Qwen3-Embedding Hugging Face](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/embeddings)

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo LICENSE para detalhes.

## üí° Suporte

- **Issues**: Abra uma issue no GitHub
- **Discuss√µes**: Use GitHub Discussions
- **Chat**: Discord ou Telegram da comunidade

---

**Desenvolvido com ‚ù§Ô∏è para facilitar o uso de embeddings avan√ßados** 