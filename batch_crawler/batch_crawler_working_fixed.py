#!/usr/bin/env python3
"""
Batch Crawler Working Fixed - Usa stdio MCP corretamente

Este script usa o m√©todo que REALMENTE funciona: subprocess + stdio + JSON-RPC 2.0
N√£o tenta usar HTTP REST que n√£o existe no FastMCP.
"""

import asyncio
import json
import logging
import os
import sys
import time
import subprocess
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
from urllib.parse import urlparse


class BatchCrawlerWorkingFixed:
    """Cliente que funciona usando subprocess + stdio + JSON-RPC."""
    
    def __init__(self, delay: float = 1.0):
        self.delay = delay
        self.server_script = None
        
        # Setup logging
        self.logger = logging.getLogger(__name__)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
        
        # Encontrar o script do servidor MCP
        self._find_server_script()
    
    def _find_server_script(self):
        """Encontra o script do servidor MCP."""
        current_dir = Path(__file__).parent
        possible_paths = [
            current_dir / ".." / "src" / "crawl4ai_mcp.py",
            current_dir.parent / "src" / "crawl4ai_mcp.py",
            Path("../src/crawl4ai_mcp.py"),
            Path("src/crawl4ai_mcp.py"),
            Path("crawl4ai_mcp.py")
        ]
        
        for path in possible_paths:
            if path.exists():
                self.server_script = str(path.resolve())
                self.logger.info(f"Servidor MCP encontrado: {self.server_script}")
                return
        
        self.logger.error("Script do servidor MCP n√£o encontrado!")
        self.server_script = None
    
    def test_connection(self) -> bool:
        """Testa se podemos executar o servidor MCP."""
        if not self.server_script:
            self.logger.error("Script do servidor n√£o encontrado")
            return False
        
        try:
            # Testar se podemos executar o script
            import subprocess
            result = subprocess.run([
                sys.executable, self.server_script, "--help"
            ], capture_output=True, timeout=10, env={**os.environ, "TRANSPORT": "stdio"})
            
            # Se n√£o der erro de import, √© bom sinal
            return True
            
        except Exception as e:
            self.logger.error(f"Erro ao testar servidor: {e}")
            return False
    
    async def crawl_single_url_stdio(self, url: str) -> Dict[str, Any]:
        """Crawl uma URL usando subprocess com stdio MCP."""
        start_time = time.time()
        
        try:
            self.logger.info(f"Iniciando crawl via stdio: {url}")
            
            # Preparar mensagem JSON-RPC 2.0 
            request = {
                "jsonrpc": "2.0",
                "id": int(time.time() * 1000),
                "method": "tools/call",
                "params": {
                    "name": "crawl_single_page",
                    "arguments": {
                        "url": url
                    }
                }
            }
            
            # Ambiente para for√ßar modo stdio
            env = os.environ.copy()
            env["TRANSPORT"] = "stdio"
            
            # Executar servidor MCP via subprocess com stdio
            process = await asyncio.create_subprocess_exec(
                sys.executable, self.server_script,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )
            
            # Enviar requisi√ß√£o JSON-RPC
            request_json = json.dumps(request) + "\n"
            
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(input=request_json.encode('utf-8')),
                    timeout=180.0  # 3 minutos timeout
                )
            except asyncio.TimeoutError:
                process.kill()
                await process.wait()
                raise Exception("Timeout na comunica√ß√£o com servidor MCP")
            
            processing_time = time.time() - start_time
            
            if process.returncode == 0:
                # Parse da resposta JSON-RPC
                try:
                    response_text = stdout.decode('utf-8').strip()
                    
                    # √Äs vezes o FastMCP retorna m√∫ltiplas linhas, pegar a √∫ltima v√°lida
                    lines = response_text.split('\n')
                    response_data = None
                    
                    for line in reversed(lines):
                        if line.strip():
                            try:
                                response_data = json.loads(line.strip())
                                break
                            except json.JSONDecodeError:
                                continue
                    
                    if not response_data:
                        raise Exception("Resposta JSON inv√°lida")
                    
                    # Verificar se √© uma resposta de erro JSON-RPC
                    if "error" in response_data:
                        error_msg = response_data["error"].get("message", "Erro desconhecido")
                        raise Exception(f"Erro MCP: {error_msg}")
                    
                    # Extrair resultado da resposta JSON-RPC
                    result_data = response_data.get("result")
                    if not result_data:
                        raise Exception("Resposta MCP sem campo 'result'")
                    
                    self.logger.info(f"Crawl conclu√≠do para {url} - {processing_time:.2f}s")
                    
                    return {
                        "success": True,
                        "url": url,
                        "data": result_data,
                        "processing_time": processing_time,
                        "timestamp": datetime.now().isoformat(),
                        "method": "stdio-mcp"
                    }
                    
                except json.JSONDecodeError as e:
                    self.logger.error(f"Erro ao decodificar JSON para {url}: {e}")
                    self.logger.error(f"Resposta recebida: {stdout.decode('utf-8')[:500]}")
                    raise Exception(f"Resposta JSON inv√°lida: {e}")
                    
            else:
                # Processo falhou
                error_output = stderr.decode('utf-8')
                self.logger.error(f"Processo falhou para {url}: {error_output}")
                raise Exception(f"Processo retornou c√≥digo {process.returncode}: {error_output}")
                
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"Erro ao processar {url}: {e}")
            
            return {
                "success": False,
                "url": url,
                "error": str(e),
                "processing_time": processing_time,
                "timestamp": datetime.now().isoformat()
            }
    
    def load_urls(self, file_path: str) -> List[str]:
        """Carrega URLs de um arquivo."""
        urls = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    
                    # Pular linhas vazias e coment√°rios
                    if not line or line.startswith('#'):
                        continue
                    
                    # Validar URL b√°sica
                    if not line.startswith(('http://', 'https://')):
                        self.logger.warning(f"Linha {line_num}: URL inv√°lida: {line}")
                        continue
                    
                    try:
                        parsed = urlparse(line)
                        if not parsed.netloc:
                            self.logger.warning(f"Linha {line_num}: URL malformada: {line}")
                            continue
                        urls.append(line)
                    except Exception as e:
                        self.logger.warning(f"Linha {line_num}: Erro ao analisar URL {line}: {e}")
                        continue
                        
            self.logger.info(f"Total de URLs v√°lidas carregadas: {len(urls)}")
            return urls
            
        except FileNotFoundError:
            self.logger.error(f"Arquivo n√£o encontrado: {file_path}")
            return []
        except Exception as e:
            self.logger.error(f"Erro ao carregar arquivo {file_path}: {e}")
            return []
    
    async def process_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """Processa lista de URLs sequencialmente."""
        results = []
        total = len(urls)
        
        for i, url in enumerate(urls, 1):
            self.logger.info(f"Processando {i}/{total}: {url}")
            
            result = await self.crawl_single_url_stdio(url)
            results.append(result)
            
            # Log resultado
            if result["success"]:
                self.logger.info(f"[OK] {url} - {result['processing_time']:.2f}s")
            else:
                self.logger.error(f"[ERRO] {url} - {result['error']}")
            
            # Delay entre requisi√ß√µes (exceto na √∫ltima)
            if i < total and self.delay > 0:
                await asyncio.sleep(self.delay)
        
        return results
    
    async def run(self, urls_file: str, output_file: str) -> bool:
        """Executa o crawler batch."""
        # Testar se podemos executar o servidor
        if not self.test_connection():
            self.logger.error("N√£o √© poss√≠vel executar o servidor MCP")
            return False
        
        # Carregar URLs
        self.logger.info(f"Lendo URLs de: {urls_file}")
        urls = self.load_urls(urls_file)
        
        if not urls:
            self.logger.error("Nenhuma URL v√°lida encontrada")
            return False
        
        # Processar URLs
        self.logger.info(f"Iniciando processamento de {len(urls)} URLs via stdio")
        results = await self.process_urls(urls)
        
        # Salvar resultados
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            # Estat√≠sticas
            successful = sum(1 for r in results if r["success"])
            failed = len(results) - successful
            
            self.logger.info("========================================")
            self.logger.info("Resultado Final:")
            self.logger.info(f"  URLs processadas: {len(results)}")
            self.logger.info(f"  Sucessos: {successful}")
            self.logger.info(f"  Falhas: {failed}")
            self.logger.info(f"  Arquivo de sa√≠da: {output_file}")
            self.logger.info("========================================")
            
            return successful > 0
            
        except Exception as e:
            self.logger.error(f"Erro ao salvar resultados: {e}")
            return False


def interactive_mode():
    """Modo interativo."""
    print("=" * 50)
    print(" Batch Crawler Working Fixed - MCP stdio")
    print("=" * 50)
    
    # Listar arquivos de URL dispon√≠veis
    current_dir = Path(__file__).parent
    url_files = list(current_dir.glob("*.txt"))
    
    if url_files:
        print("\nArquivos de URL dispon√≠veis:\n")
        for i, file_path in enumerate(url_files, 1):
            print(f"  {i}. {file_path.name}")
        
        print(f"\nOp√ß√µes:")
        print(f"  1-{len(url_files)}. Selecionar arquivo listado")
        print(f"  m. Digitar caminho manualmente")
        print(f"  q. Sair")
        
        while True:
            choice = input("\nEscolha uma op√ß√£o: ").strip().lower()
            
            if choice == 'q':
                return None
            elif choice == 'm':
                urls_file = input("Digite o caminho do arquivo: ").strip()
                break
            elif choice.isdigit():
                idx = int(choice) - 1
                if 0 <= idx < len(url_files):
                    urls_file = str(url_files[idx])
                    break
                else:
                    print("Op√ß√£o inv√°lida!")
            else:
                print("Op√ß√£o inv√°lida!")
    else:
        urls_file = input("Digite o caminho do arquivo de URLs: ").strip()
    
    # Configura√ß√µes
    output = input("Arquivo de sa√≠da [results.json]: ").strip() or "results.json"
    delay_str = input("Delay entre requisi√ß√µes [1.0]: ").strip()
    delay = float(delay_str) if delay_str else 1.0
    
    return {
        'urls_file': urls_file,
        'output': output,
        'delay': delay
    }


async def main():
    """Fun√ß√£o principal."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Batch Crawler Working Fixed")
    parser.add_argument("urls_file", nargs="?", help="Arquivo com URLs")
    parser.add_argument("-o", "--output", default="results.json", help="Arquivo de sa√≠da")
    parser.add_argument("-d", "--delay", type=float, default=1.0, help="Delay entre requests")
    parser.add_argument("-i", "--interactive", action="store_true", help="Modo interativo")
    
    args = parser.parse_args()
    
    # Modo interativo se necess√°rio
    if args.interactive or not args.urls_file:
        config = interactive_mode()
        if not config:
            return
        
        args.urls_file = config['urls_file']
        args.output = config['output']
        args.delay = config['delay']
    
    if not args.urls_file:
        print("Erro: arquivo de URLs n√£o especificado")
        return
    
    print(f"\nüöÄ Iniciando crawling...")
    print("=" * 50)
    
    # Executar crawler
    crawler = BatchCrawlerWorkingFixed(args.delay)
    success = await crawler.run(args.urls_file, args.output)
    
    if success:
        print("\n‚úÖ Crawling conclu√≠do com sucesso!")
    else:
        print("\n‚ùå Crawling falhou")
        sys.exit(1)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nüõë Opera√ß√£o cancelada pelo usu√°rio")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Erro fatal: {e}")
        sys.exit(1)