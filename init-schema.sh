#!/bin/bash

# Script to initialize Supabase schema using curl
# This script will be executed when the Docker container starts

echo "üöÄ Initializing Supabase schema..."

# Install required dependencies
echo "üì¶ Installing dependencies..."
apt-get update -qq && apt-get install -y -qq curl jq

# Check if required environment variables are set
if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_SERVICE_KEY" ]; then
    echo "‚ùå Error: SUPABASE_URL and SUPABASE_SERVICE_KEY must be set"
    exit 1
fi

# Check if SQL schema file exists
if [ ! -f "/app/crawled_pages.sql" ]; then
    echo "‚ùå Error: Schema file not found at /app/crawled_pages.sql"
    exit 1
fi

# Check if sources table exists
echo "üîç Checking if sources table exists..."
check_response=$(curl -s -X GET \
    "${SUPABASE_URL}/rest/v1/sources?select=source_id&limit=1" \
    -H "apikey: ${SUPABASE_SERVICE_KEY}" \
    -H "Authorization: Bearer ${SUPABASE_SERVICE_KEY}")

echo "API Response: $check_response"

if echo "$check_response" | grep -q -E "(error|code|42P01|does not exist)"; then
    echo "üìã Sources table doesn't exist. Creating full schema..."
    
    # Read the entire SQL schema file
    sql_content=$(cat /app/crawled_pages.sql)
    
    # Execute the full SQL schema
    echo "üîß Executing crawled_pages.sql schema..."
    response=$(curl -s -X POST \
        "${SUPABASE_URL}/rest/v1/rpc/exec_sql" \
        -H "apikey: ${SUPABASE_SERVICE_KEY}" \
        -H "Authorization: Bearer ${SUPABASE_SERVICE_KEY}" \
        -H "Content-Type: application/json" \
        -d "{\"sql\": $(echo "$sql_content" | jq -Rs .)}")
    
    echo "Schema execution response: $response"
    
    if echo "$response" | grep -q -E "(error|code|404)"; then
        echo "‚ùå Error executing schema: $response"
        echo "üí° The exec_sql function is not available in Supabase."
        echo "üìã MANUAL ACTION REQUIRED:"
        echo "   1. Go to https://supabase.com/dashboard/project/uvwrdypcprzirkffjrhn"
        echo "   2. Click on 'SQL Editor' in the sidebar"
        echo "   3. Copy and paste the content of 'crawled_pages.sql'"
        echo "   4. Execute the SQL to create the required tables"
        echo "‚ö†Ô∏è  Server will start but crawling will fail until you create the tables manually."
    else
        echo "‚úÖ Schema initialization completed successfully!"
    fi
else
    echo "‚úÖ Sources table already exists. Skipping schema initialization."
fi

echo "üéâ Schema initialization process finished."